# 1. Project Overview


## Part I
   
The objective of this project is to create a star/snowflake model data warehouse from the Chinook OLTP
database, in order to enable study of the data by business analysts, and create a sample report
demonstrating the functionality of the data warehouse in combination with a data analysis and
visualization tool such as Power BI.

## Part II

The objective of this project is to analyze the sales performance of employees in a simulated digital
music store environment represented by the Chinook database. Students will extract relevant data,
preprocess it, and utilize machine learning techniques to classify employees based on their sales
performance. This project aims to help students understand how to derive insights from data and apply
predictive modeling.


# 2. Learning Outcomes

 
## Part I

- Understand and implement the process of designing a DW, based on an existing OLTP
database.
- Implement ETL from an OLTP database to a data warehouse using SQL Server (Database
or SSIS).
- Understand the usefulness of a Data Warehouse in combination with an analytics tool.
- Implement basic reports using Power BI.

## Part II

- Understand how to extract and manipulate data from a relational database.
- Gain experience in feature engineering and data preprocessing.
- Learn to implement classification techniques for performance analysis.
- Develop skills in evaluating model performance using K-Fold Cross-Validation and hyperparameter tuning.
- Interpret results and derive actionable insights. 


# 3. Project Steps


## Part I
   
### A. Data Warehouse Design
Create a DW to host the data found in the original Chinook OLTP database. The DW should
follow the star or snowflake schema principles.

### B. Extract, Transform and Load Data
Create SQL scripts or an SSIS package to transfer data from the OLTP Chinook database to the
DW you created.

### C. Create a Report
Create a report showcasing the basic functionality of your DW using Power BI. The report can
showcase any data you consider as the most useful from your DW. Optionally, you may also
include data from Part 2 of the project in your report, in addition to data from your DW, if you
believe they add useful context.


## Part II

### A. Data Extraction

*Objective: Extract relevant sales performance data for employees from the Chinook database.*

<ins>Identify Relevant Tables:</ins>
- Employee: Contains employee details, including EmployeeId, FirstName, LastName, and HireDate.
- Customer: Contains customer information linked to employees through SupportRepId.
- Invoice: Contains records of purchases made by customers, including InvoiceId, CustomerId, and Total 2

<ins>Write and Execute an SQL Query:</ins>
- Create a SQL query that joins the Employee, Customer, and Invoice tables to calculate metrics such as total revenue generated, number of invoices, and average revenue per customer.
- Export the result into a format suitable for analysis, such as CSV or Excel.

### B. Data Preprocessing & Feature Engineering

*Objective: Clean and prepare the extracted data for analysis.*

- Load Data
- Clean and Transform Data
- Feature Engineering: Create new features that may be beneficial for analysis, such as tenure:
calculate how long each employee has been with the company based on their hire date.

### C. Define Performance Labels

*Objective: Categorize employees into performance levels based on revenue generated.*

- Define categories such as "High Performer," "Average Performer," and "Low Performer" based on the
total revenue generated by each employee. Use quantiles or fixed thresholds to establish boundaries for
these categories.

### D. Machine Learning Pipeline

*Objective:* Implement a machine learning classification model to predict employee performance.

<ins>Data Splitting:</ins>
- Divide the dataset into features (inputs) and target labels (employee performance
categories).
- Split the data into training and testing sets to evaluate model performance effectively.

<ins>Spot-Check Algorithms:</ins>
- Perform an initial evaluation of multiple classification algorithms (e.g., Decision Tree, Random Forest, Support Vector Machine, Logistic Regression) using a simple evaluation metric (e.g., accuracy).
- Select a few of the best-performing algorithms for further analysis, based on the spot-check results.

<ins>K-Fold Cross-Validation:</ins>
- Implement K-Fold Cross-Validation on the selected models to assess their performance more robustly.

<ins>Hyperparameter Tuning:</ins>
- Use techniques like Grid Search or Random Search to find the best hyperparameters for the chosen models.

<ins>Training the Model:</ins>
- Train the selected model on the training dataset, using the optimal hyperparameters identified in the tuning phase.

<ins>Model Evaluation:</ins>
- Evaluate the modelâ€™s performance on the testing dataset using metrics such as accuracy, precision, recall, and F1-score.
- Analyze the model's confusion matrix to understand misclassifications.

### E. Insights and Presentation

*Objective: Analyze the results and derive insights from the model.*

<ins>Feature Importance Analysis:</ins>
- Assess which features had the most significant impact on predicting employee performance.
- Compile a presentation summarizing the methodology, findings, and insights gained from the analysis.
- Present findings using visualizations to illustrate the relationships between features and performance.
- Discuss potential implications for employee management and sales strategies based on the results.


# 4. Deliverables

## Part I

- A script to create the Data Warehouse.
- A script or SSIS package to load the DW from the OLTP database.
- A backup (.bak) of the final loaded DW.
- A Power BI report file (.pbix).


## Part II

- A cleaned and preprocessed dataset ready for analysis.
- A machine learning model with a performance evaluation report.
- A complete comprehensive dashboard of the important measures and metrics
- A final presentation documenting the entire process, findings, and recommendations for improving employee sales performance.
